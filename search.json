[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing any of the following contacts:\n\nRahiq Raees — rahiqraees10@gmail.com\n\nEduardo Sanchez — ed.rasanmar@gmail.com\n\nManikanth Goud — manikanthgoud27@gmail.com\n\nYonas Gebre — yonasgebre08@gmail.com\n\nAll complaints will be reviewed and investigated promptly, fairly, and with appropriate confidentiality.\nCommunity leaders are obligated to respect the privacy and security of the reporter of any incident and will take reasonable steps to protect reporters from retaliation.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement by emailing any of the following contacts:\n\nRahiq Raees — rahiqraees10@gmail.com\n\nEduardo Sanchez — ed.rasanmar@gmail.com\n\nManikanth Goud — manikanthgoud27@gmail.com\n\nYonas Gebre — yonasgebre08@gmail.com\n\nAll complaints will be reviewed and investigated promptly, fairly, and with appropriate confidentiality.\nCommunity leaders are obligated to respect the privacy and security of the reporter of any incident and will take reasonable steps to protect reporters from retaliation."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "reference/validate_contract.html",
    "href": "reference/validate_contract.html",
    "title": "validate_contract",
    "section": "",
    "text": "validate_contract\n\n\n\n\n\nName\nDescription\n\n\n\n\nvalidate_contract\nValidate a pandas DataFrame against a predefined data contract.\n\n\n\n\n\nvalidate_contract.validate_contract(df, contract, strict=True)\nValidate a pandas DataFrame against a predefined data contract.\nThis function validates an input DataFrame by comparing it against a contract that defines expected columns, data types, missingness thresholds, numeric value limits, and allowed categorical values. All columns defined in the contract are treated as required. Validation results are returned as a collection of structured issues describing any detected violations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nThe DataFrame to be validated.\nrequired\n\n\ncontract\nContract\nA data contract defining the expected columns and validation rules for each column, including: - expected data type (as a string), - maximum allowed fraction of missing values, - minimum and maximum values for numeric columns, - allowed categorical values.\nrequired\n\n\nstrict\nbool\nIf True, the presence of extra columns in the DataFrame that are not defined in the contract is reported as validation issues. If False, extra columns are ignored.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValidationResult\nAn object containing: - a boolean flag (ok) indicating whether validation succeeded, - a list of Issue objects describing all detected validation problems.\n\n\n\n\n\n\nThe function checks for missing required columns, unexpected extra columns (when strict mode is enabled), data type mismatches, missingness violations, numeric range violations, and invalid or unseen categorical values.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n&gt;&gt;&gt; result = validate_contract(df, contract)\n&gt;&gt;&gt; result.ok\nTrue"
  },
  {
    "objectID": "reference/validate_contract.html#functions",
    "href": "reference/validate_contract.html#functions",
    "title": "validate_contract",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nvalidate_contract\nValidate a pandas DataFrame against a predefined data contract.\n\n\n\n\n\nvalidate_contract.validate_contract(df, contract, strict=True)\nValidate a pandas DataFrame against a predefined data contract.\nThis function validates an input DataFrame by comparing it against a contract that defines expected columns, data types, missingness thresholds, numeric value limits, and allowed categorical values. All columns defined in the contract are treated as required. Validation results are returned as a collection of structured issues describing any detected violations.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.DataFrame\nThe DataFrame to be validated.\nrequired\n\n\ncontract\nContract\nA data contract defining the expected columns and validation rules for each column, including: - expected data type (as a string), - maximum allowed fraction of missing values, - minimum and maximum values for numeric columns, - allowed categorical values.\nrequired\n\n\nstrict\nbool\nIf True, the presence of extra columns in the DataFrame that are not defined in the contract is reported as validation issues. If False, extra columns are ignored.\nTrue\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValidationResult\nAn object containing: - a boolean flag (ok) indicating whether validation succeeded, - a list of Issue objects describing all detected validation problems.\n\n\n\n\n\n\nThe function checks for missing required columns, unexpected extra columns (when strict mode is enabled), data type mismatches, missingness violations, numeric range violations, and invalid or unseen categorical values.\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n&gt;&gt;&gt; result = validate_contract(df, contract)\n&gt;&gt;&gt; result.ok\nTrue"
  },
  {
    "objectID": "reference/types.DriftReport.html",
    "href": "reference/types.DriftReport.html",
    "title": "types.DriftReport",
    "section": "",
    "text": "types.DriftReport\ntypes.DriftReport(\n    added_columns=set(),\n    removed_columns=set(),\n    dtype_changes=dict(),\n    range_changes=set(),\n    category_changes=set(),\n    missingness_changes=dict(),\n)\nOutput of compare_contracts().\nKeep it tiny: only what changed between two contracts."
  },
  {
    "objectID": "reference/types.ColumnRule.html",
    "href": "reference/types.ColumnRule.html",
    "title": "types.ColumnRule",
    "section": "",
    "text": "types.ColumnRule\ntypes.ColumnRule(\n    dtype,\n    max_missing_frac=0.0,\n    min_value=None,\n    max_value=None,\n    allowed_values=None,\n)\nMinimal per-column expectations.\ndtype: simple string such as “int”, “float”, “string”, “bool”, “datetime”, “category” max_missing_frac: fraction of missing values allowed in [0, 1] min_value/max_value: numeric bounds (optional) allowed_values: allowed categorical values (optional)"
  },
  {
    "objectID": "reference/compare_contracts.html",
    "href": "reference/compare_contracts.html",
    "title": "compare_contracts",
    "section": "",
    "text": "compare_contracts\n\n\n\n\n\nName\nDescription\n\n\n\n\ncompare_contracts\nCompare two data contracts to detect schema and constraint drift.\n\n\n\n\n\ncompare_contracts.compare_contracts(contract_a, contract_b)\nCompare two data contracts to detect schema and constraint drift.\nThis function compares a reference (baseline) contract against an observed (latest) contract and reports differences in: - schema: added/removed columns and dtype changes - constraints: numeric bound changes, categorical domain changes, and missingness threshold changes\nThe comparison is directional: - “added” means present in contract_b but not in contract_a - “removed” means present in contract_a but not in contract_b - “old” refers to contract_a and “new” refers to contract_b\n\n\n\nAdded columns: column in contract_b.columns but not in contract_a.columns\nRemoved columns: column in contract_a.columns but not in contract_b.columns\nDtype changes: for columns present in both contracts, ColumnRule.dtype differs (reported as (old_dtype, new_dtype))\nRange changes (numeric bounds): for columns present in both contracts, min_value and/or max_value differs (only meaningful when numeric bounds are provided; this function compares the stored contract values, not raw data)\nCategory changes: for columns present in both contracts, allowed_values differs\nMissingness changes: for columns present in both contracts, max_missing_frac differs (reported as (old_max_missing_frac, new_max_missing_frac))\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncontract_a\nContract\nReference contract representing the expected schema and constraints.\nrequired\n\n\ncontract_b\nContract\nObserved contract representing the latest schema and constraints.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDriftReport\nA report containing only detected differences between the two contracts: - added_columns, removed_columns - dtype_changes (col -&gt; (old, new)) - range_changes (set of columns whose min/max changed) - category_changes (set of columns whose allowed_values changed) - missingness_changes (col -&gt; (old, new))\n\n\n\n\n\n\nThis function compares contract metadata only and does not inspect raw data. Drift is evaluated only for columns that exist in both contracts, except for added or removed columns detected via column name differences. Handling of optional fields (min_value, max_value, allowed_values) is implementation- defined; document your chosen rule if it matters for users.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf contract_a or contract_b is not a Contract instance, or if a column rule is not a ColumnRule instance.\n\n\n\nValueError\nIf max_missing_frac is non-numeric, outside [0, 1], or if min_value exceeds max_value.\n\n\n\n\n\n\n&gt;&gt;&gt; report = compare_contracts(contract_a, contract_b)\n&gt;&gt;&gt; report.has_drift\nTrue\n&gt;&gt;&gt; report.missingness_changes\n{'age': (0.05, 0.20)}"
  },
  {
    "objectID": "reference/compare_contracts.html#functions",
    "href": "reference/compare_contracts.html#functions",
    "title": "compare_contracts",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncompare_contracts\nCompare two data contracts to detect schema and constraint drift.\n\n\n\n\n\ncompare_contracts.compare_contracts(contract_a, contract_b)\nCompare two data contracts to detect schema and constraint drift.\nThis function compares a reference (baseline) contract against an observed (latest) contract and reports differences in: - schema: added/removed columns and dtype changes - constraints: numeric bound changes, categorical domain changes, and missingness threshold changes\nThe comparison is directional: - “added” means present in contract_b but not in contract_a - “removed” means present in contract_a but not in contract_b - “old” refers to contract_a and “new” refers to contract_b\n\n\n\nAdded columns: column in contract_b.columns but not in contract_a.columns\nRemoved columns: column in contract_a.columns but not in contract_b.columns\nDtype changes: for columns present in both contracts, ColumnRule.dtype differs (reported as (old_dtype, new_dtype))\nRange changes (numeric bounds): for columns present in both contracts, min_value and/or max_value differs (only meaningful when numeric bounds are provided; this function compares the stored contract values, not raw data)\nCategory changes: for columns present in both contracts, allowed_values differs\nMissingness changes: for columns present in both contracts, max_missing_frac differs (reported as (old_max_missing_frac, new_max_missing_frac))\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncontract_a\nContract\nReference contract representing the expected schema and constraints.\nrequired\n\n\ncontract_b\nContract\nObserved contract representing the latest schema and constraints.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nDriftReport\nA report containing only detected differences between the two contracts: - added_columns, removed_columns - dtype_changes (col -&gt; (old, new)) - range_changes (set of columns whose min/max changed) - category_changes (set of columns whose allowed_values changed) - missingness_changes (col -&gt; (old, new))\n\n\n\n\n\n\nThis function compares contract metadata only and does not inspect raw data. Drift is evaluated only for columns that exist in both contracts, except for added or removed columns detected via column name differences. Handling of optional fields (min_value, max_value, allowed_values) is implementation- defined; document your chosen rule if it matters for users.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf contract_a or contract_b is not a Contract instance, or if a column rule is not a ColumnRule instance.\n\n\n\nValueError\nIf max_missing_frac is non-numeric, outside [0, 1], or if min_value exceeds max_value.\n\n\n\n\n\n\n&gt;&gt;&gt; report = compare_contracts(contract_a, contract_b)\n&gt;&gt;&gt; report.has_drift\nTrue\n&gt;&gt;&gt; report.missingness_changes\n{'age': (0.05, 0.20)}"
  },
  {
    "objectID": "reference/types.ValidationResult.html",
    "href": "reference/types.ValidationResult.html",
    "title": "types.ValidationResult",
    "section": "",
    "text": "types.ValidationResult\ntypes.ValidationResult(ok, issues=list())\nOutput of validate_contract()."
  },
  {
    "objectID": "reference/types.Issue.html",
    "href": "reference/types.Issue.html",
    "title": "types.Issue",
    "section": "",
    "text": "types.Issue\ntypes.Issue(kind, message, column=None, observed=None, expected=None)\nA single validation issue.\nkind examples: - “missing_column”, “extra_column” - “dtype”, “missingness”, “range”, “category” column=None for dataset-level issues."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "Contributions of all kinds are welcome here, and they are greatly appreciated! Every little bit helps, and credit will always be given.\n\n\nYou can contribute in many ways, for example:\n\nContributing\n\nExample Contributions\n\nReport Bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\nGet Started!\n\nPull Request Guidelines\n\nDevelopment Tools, Infrastructure, and Practices\n\nDevelopment Tools\nGitHub Infrastructure\nOrganizational and Collaboration Practices\n\nScaling Considerations\n\n\n\n\nReport bugs at https://github.com/UBC-MDS/DSCI_524_G26_Data_Validation/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\npyos_data_validation could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/DSCI_524_G26_Data_Validation/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time.\n\n\n\n\nFollow these steps to set up the project locally and start contributing.\n\nFork the repository on GitHub:\nhttps://github.com/UBC-MDS/DSCI_524_G26_Data_Validation\nClone your fork locally:\ngit clone git@github.com:your_github_username/DSCI_524_G26_Data_Validation.git\ncd DSCI_524_G26_Data_Validation\nCreate and activate the Conda environment using the provided environment.yml file (this environment already includes Hatch):\nconda env create -f environment.yml\nconda activate pyos_data_validation\nCreate a new branch from the default branch (main).\nUse fix/ or feat/ as a prefix for your branch name:\ngit checkout main\ngit pull\ngit checkout -b fix-short-description\nMake your changes locally. When finished, run the test suite:\nhatch run test:run\nCommit your changes and push your branch to GitHub.\nPlease use semantic commit messages:\ngit add .\ngit commit -m \"fix: short description of change\"\ngit push -u origin fix-short-description\nOpen a pull request against the main branch using the link shown after pushing.\n\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging.\n\n\n\n\n\n\nThis project applies modern Python development workflows and collaborative practices learned in DSCI 524, with a strong emphasis on reproducibility, automation, and code quality.\n\n\n\nHatch is used for environment management, testing, and task execution. This ensures consistent developer environments and simplifies common workflows such as running tests and checks.\nRuff is used for formatting and linting to enforce PEP 8–compliant, readable code and to provide fast feedback during development.\nPytest is used for automated testing to validate correctness and prevent regressions as the codebase evolves.\nQuartodoc + Quarto are used to generate API documentation directly from docstrings, ensuring documentation stays closely aligned with the code.\n\n\n\n\n\nGitHub Issues are used to track bugs, feature requests, and documentation improvements, with labels (bug, enhancement, help wanted) to organize work and encourage contributions.\nPull Requests are the primary mechanism for code review, discussion, and integration. All changes are reviewed before merging.\nGitHub Actions (CI) automatically run tests, formatting checks, and build steps on every pull request to main, ensuring consistent quality standards and preventing broken code from being merged.\nBranch-based development is used, with feature and fix branches (feat/*, fix/*) to keep the main branch stable.\n\n\n\n\n\nSemantic commit messages (Conventional Commits) improve readability of the project history and support changelog generation.\nConsistent docstring standards ensure functions are easy to understand and maintain, especially for new contributors.\nClear contribution guidelines lower the barrier to entry for contributors and help standardize collaboration across the team.\n\n\n\n\n\n\nIf this project (or a similar one) were to scale to a larger user base or contributor community, the following tools and practices would be adopted or expanded:\n\nStricter CI gates, such as required test coverage thresholds and branch protection rules, to maintain code quality at scale.\nDependency monitoring tools (e.g., Dependabot) to keep dependencies secure and up to date.\nPre-commit hooks to catch formatting, linting, and documentation issues earlier in the development cycle.\nExpanded documentation and examples, including tutorials and usage guides, to support a broader audience.\nIssue and PR templates refinement, ensuring high-quality reports and consistent reviews as contribution volume grows.\n\nThese tools and practices help ensure that the project remains maintainable, reliable, and welcoming as it scales in complexity and community size."
  },
  {
    "objectID": "CONTRIBUTING.html#example-contributions",
    "href": "CONTRIBUTING.html#example-contributions",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute in many ways, for example:\n\nContributing\n\nExample Contributions\n\nReport Bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\nGet Started!\n\nPull Request Guidelines\n\nDevelopment Tools, Infrastructure, and Practices\n\nDevelopment Tools\nGitHub Infrastructure\nOrganizational and Collaboration Practices\n\nScaling Considerations\n\n\n\n\nReport bugs at https://github.com/UBC-MDS/DSCI_524_G26_Data_Validation/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\npyos_data_validation could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/DSCI_524_G26_Data_Validation/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time."
  },
  {
    "objectID": "CONTRIBUTING.html#get-started",
    "href": "CONTRIBUTING.html#get-started",
    "title": "Contributing",
    "section": "",
    "text": "Follow these steps to set up the project locally and start contributing.\n\nFork the repository on GitHub:\nhttps://github.com/UBC-MDS/DSCI_524_G26_Data_Validation\nClone your fork locally:\ngit clone git@github.com:your_github_username/DSCI_524_G26_Data_Validation.git\ncd DSCI_524_G26_Data_Validation\nCreate and activate the Conda environment using the provided environment.yml file (this environment already includes Hatch):\nconda env create -f environment.yml\nconda activate pyos_data_validation\nCreate a new branch from the default branch (main).\nUse fix/ or feat/ as a prefix for your branch name:\ngit checkout main\ngit pull\ngit checkout -b fix-short-description\nMake your changes locally. When finished, run the test suite:\nhatch run test:run\nCommit your changes and push your branch to GitHub.\nPlease use semantic commit messages:\ngit add .\ngit commit -m \"fix: short description of change\"\ngit push -u origin fix-short-description\nOpen a pull request against the main branch using the link shown after pushing.\n\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging."
  },
  {
    "objectID": "CONTRIBUTING.html#development-tools-infrastructure-and-practices",
    "href": "CONTRIBUTING.html#development-tools-infrastructure-and-practices",
    "title": "Contributing",
    "section": "",
    "text": "This project applies modern Python development workflows and collaborative practices learned in DSCI 524, with a strong emphasis on reproducibility, automation, and code quality.\n\n\n\nHatch is used for environment management, testing, and task execution. This ensures consistent developer environments and simplifies common workflows such as running tests and checks.\nRuff is used for formatting and linting to enforce PEP 8–compliant, readable code and to provide fast feedback during development.\nPytest is used for automated testing to validate correctness and prevent regressions as the codebase evolves.\nQuartodoc + Quarto are used to generate API documentation directly from docstrings, ensuring documentation stays closely aligned with the code.\n\n\n\n\n\nGitHub Issues are used to track bugs, feature requests, and documentation improvements, with labels (bug, enhancement, help wanted) to organize work and encourage contributions.\nPull Requests are the primary mechanism for code review, discussion, and integration. All changes are reviewed before merging.\nGitHub Actions (CI) automatically run tests, formatting checks, and build steps on every pull request to main, ensuring consistent quality standards and preventing broken code from being merged.\nBranch-based development is used, with feature and fix branches (feat/*, fix/*) to keep the main branch stable.\n\n\n\n\n\nSemantic commit messages (Conventional Commits) improve readability of the project history and support changelog generation.\nConsistent docstring standards ensure functions are easy to understand and maintain, especially for new contributors.\nClear contribution guidelines lower the barrier to entry for contributors and help standardize collaboration across the team."
  },
  {
    "objectID": "CONTRIBUTING.html#scaling-considerations",
    "href": "CONTRIBUTING.html#scaling-considerations",
    "title": "Contributing",
    "section": "",
    "text": "If this project (or a similar one) were to scale to a larger user base or contributor community, the following tools and practices would be adopted or expanded:\n\nStricter CI gates, such as required test coverage thresholds and branch protection rules, to maintain code quality at scale.\nDependency monitoring tools (e.g., Dependabot) to keep dependencies secure and up to date.\nPre-commit hooks to catch formatting, linting, and documentation issues earlier in the development cycle.\nExpanded documentation and examples, including tutorials and usage guides, to support a broader audience.\nIssue and PR templates refinement, ensuring high-quality reports and consistent reviews as contribution volume grows.\n\nThese tools and practices help ensure that the project remains maintainable, reliable, and welcoming as it scales in complexity and community size."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\n\n\nAdded compare_contracts function for detecting schema and constraint drift between data contracts.\nAdded support for missingness drift detection (max_missing_frac).\nAdded has_drift convenience property to DriftReport.\n\n\n\n\n\nImproved DriftReport to include missingness-related changes.\nUpdated documentation for compare_contracts with explicit drift definitions and directionality.\nRenamed the package in documentation/metadata to pyos_data_validation.\nRemoved shell prompt and split commands inside README command snippets for easier copy/paste.\nStandardized docstrings across public APIs (consistent Notes formatting, Raises sections).\nAdded hyperlinks to Pandera, Great Expectations, and Pydantic in README comparison section.\nAdded docs to the developer guid install list for quartodoc build to work.\n\n\n\n\n\nAdded comprehensive unit tests for compare_contracts, covering schema drift, constraint drift, edge cases, and error handling.\nAdded comprehensive unit tests for validate_contract, covering edge cases, and error handling.\n\n\n\n\n\n\nFirst release"
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "Changelog",
    "section": "",
    "text": "Added compare_contracts function for detecting schema and constraint drift between data contracts.\nAdded support for missingness drift detection (max_missing_frac).\nAdded has_drift convenience property to DriftReport.\n\n\n\n\n\nImproved DriftReport to include missingness-related changes.\nUpdated documentation for compare_contracts with explicit drift definitions and directionality.\nRenamed the package in documentation/metadata to pyos_data_validation.\nRemoved shell prompt and split commands inside README command snippets for easier copy/paste.\nStandardized docstrings across public APIs (consistent Notes formatting, Raises sections).\nAdded hyperlinks to Pandera, Great Expectations, and Pydantic in README comparison section.\nAdded docs to the developer guid install list for quartodoc build to work.\n\n\n\n\n\nAdded comprehensive unit tests for compare_contracts, covering schema drift, constraint drift, edge cases, and error handling.\nAdded comprehensive unit tests for validate_contract, covering edge cases, and error handling."
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "First release"
  },
  {
    "objectID": "DEVELOPMENT.html",
    "href": "DEVELOPMENT.html",
    "title": "Development Guide",
    "section": "",
    "text": "Welcome to your shiny new package. This page will help you get started with using Hatch to manage your package.\nIf you look at your project, you will see that a pyproject.toml file. This file stores both your package configuration and settings for development tools like Hatch that you will use to work on your package.\nThis file is written using a .toml format. You can learn more about toml here. Here’s the TL&DR:\n\nEach [] section in the toml file is called a table.\nYou can nest tables with double brackets like this[[]]\nTables contain information about a element that you want to configure.\n\nWe are using Hatch as the default packaging tool. Hatch allows you to configure and run environments and scripts similar to workflow tools like tox or nox.\nHach, by default, uses virtual environments (venv) to manage environments. But you can configure it to use other environment tools.Read the hatch documentation to learn more about environments.\nFor this template, we have set up Hatch environments for you to use. At the bottom of your pyproject.toml file, notice a hatch environment section that looks like this:\n########################################\n# Hatch Environments\n########################################\nBelow is the Hatch environment to install your package. Notice that it defines pip and twine as two packages that the environment needs.\n[tool.hatch.envs.build]\ndescription = \"\"\"Test the installation the package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\nThe table below defines the scripts that you will run build and check your package.\n[tool.hatch.envs.build.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\ndetached = true\nYou can enter that environment to check it out:\n$ hatch shell build\nIf you run pip list, in the environment, twine will be there:\n$ pip list\nHatch by default, installs your package in editable mode (-e) into its virtual environments. But if detached=True is set, then it will skip installing your package into the virtual enviornment.\n\n\nBelow you see the Hatch environment test table.\ntool.hatch.envs says, “Hey, Hatch, this is the definition for an environment.” test is the name of the environment.\nThe environment below defines the dependencies that Hatch needs to install into the environment named test.\n[tool.hatch.envs.test]\ndescription = \"\"\"Run the test suite.\"\"\"\ndependencies = [\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-raises\",\n    \"pytest-randomly\",\n    \"pytest-xdist\",\n]\nTo enter a Hatch environment use:\nhatch shell environmentname\nSo you can enter the test environment above with:\nhatch shell test\n\n\n\nIf the environment has a matrix associated with it, that tells Hatch to run the test scripts across different Python versions.\n[[tool.hatch.envs.test.matrix]]\npython = [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\nIf you run hatch shell test, you will see the output below. To enter an environment with a matrix attached to it, you need to pick the Python environment version that you want to open.\n$ hatch shell test                           \nEnvironment `test` defines a matrix, choose one of the following instead:\n\ntest.py3.10\ntest.py3.11\ntest.py3.12\ntest.py3.13\nOpen the Python 3.13 environment like this:\n$ hatch shell test.py3.13\nTo leave an environment use:\n$ deactivate\n\n\n\nIn the tests section of your pyproject.toml, you will see a tool.hatch.envs.test.scripts table.\nThis table defines the commands that you want Hatch to run in the test environment. Notice that the script has one command called run.\n[tool.hatch.envs.test.scripts]\nrun = \"pytest {args:--cov=greatproject --cov-report=term-missing}\"\nTo run this script , use:\nhatch run test:run\n\nhatch run: calls Hatch and tells it that it will be running a command\ntest:run: defines the environment you want it to run (test) and defines the name of the “script” to berun.\n\nIf you have a Hatch matrix setup for tests, it will both install the necessary Python version using UV and run your tests on each version of the Python versions that you declare in the matrix table. In this case, there are 4 Python versions in the environment, so your tests will run 4 times, once in each Python version listed in the matrix table.\n@lwasser ➜ /workspaces/pyopensci-scipy25-create-python-package (main) $ hatch run test:run\n──────────────────────────────────────────────────────────────────────── test.py3.10 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.10.16, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1490740387\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.10.16-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n──────────────────────────────────────────────────────────────────────── test.py3.11 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.11.12, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1596865075\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.11.12-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n\n\n\nYou can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "DEVELOPMENT.html#build-your-package",
    "href": "DEVELOPMENT.html#build-your-package",
    "title": "Development Guide",
    "section": "",
    "text": "You can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pyos_data_validation",
    "section": "",
    "text": "pyos_data_validation is a lightweight Python package for defining, validating, and comparing data contracts for tabular datasets. A data contract captures assumptions about a dataset’s schema, data types, missingness, numeric ranges, and categorical values.\nThe package supports a simple, reproducible workflow:\n\nInfer a contract from a reference dataset\n\nValidate new data against the contract\n\nCompare contracts to detect schema or distribution drift\n\nSummarize validation failures for debugging and CI use",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "pyos_data_validation",
    "section": "",
    "text": "pyos_data_validation is a lightweight Python package for defining, validating, and comparing data contracts for tabular datasets. A data contract captures assumptions about a dataset’s schema, data types, missingness, numeric ranges, and categorical values.\nThe package supports a simple, reproducible workflow:\n\nInfer a contract from a reference dataset\n\nValidate new data against the contract\n\nCompare contracts to detect schema or distribution drift\n\nSummarize validation failures for debugging and CI use",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "pyos_data_validation",
    "section": "Installation",
    "text": "Installation\nInstall from TestPyPI:\npip install -i https://test.pypi.org/simple/ pyos-data-validation\nOr install from source for development:\ngit clone https://github.com/UBC-MDS/DSCI_524_G26_Data_Validation.git\ncd DSCI_524_G26_Data_Validation\npip install -e .",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "pyos_data_validation",
    "section": "Quick Start",
    "text": "Quick Start\nHere’s a complete workflow demonstrating all four core functions:\nimport pandas as pd\nfrom pyos_data_validation import (\n    infer_contract,\n    validate_contract,\n    compare_contracts,\n    summarize_violations,\n)\n\n# Step 1: Create training data\ntraining_data = pd.DataFrame({\n    \"age\": [25, 40, 35, 28, 45],\n    \"salary\": [50000, 75000, 62000, 55000, 80000],\n    \"department\": [\"HR\", \"Engineering\", \"HR\", \"Sales\", \"Engineering\"]\n})\n\n# Step 2: Infer a contract from the training data\ncontract = infer_contract(training_data)\nprint(\"Contract inferred successfully!\")\nprint(f\"Columns in contract: {list(contract.columns.keys())}\")\n\nValidating Data\nValidate new data against the contract:\n# Valid data - passes all checks\nvalid_data = pd.DataFrame({\n    \"age\": [30, 42],\n    \"salary\": [58000, 72000],\n    \"department\": [\"HR\", \"Sales\"]\n})\n\nresult = validate_contract(valid_data, contract)\nprint(f\"Validation passed: {result.ok}\")\nprint(f\"Issues found: {len(result.issues)}\")\nNow let’s see what happens with invalid data:\n# Invalid data - multiple violations\ninvalid_data = pd.DataFrame({\n    \"age\": [30, 55, 22],  # 55 and 22 outside range\n    \"salary\": [58000, 72000, 45000],  # 45000 below minimum\n    \"department\": [\"HR\", \"Sales\", \"Marketing\"],  # Marketing not in contract\n    \"bonus\": [5000, 8000, 3000]  # Extra column\n})\n\nresult = validate_contract(invalid_data, contract)\nprint(f\"Validation passed: {result.ok}\")\nprint(f\"Issues found: {len(result.issues)}\")\n\n# Show first few issues\nfor issue in result.issues[:3]:\n    print(f\"  - {issue.column}: {issue.kind}\")\n\n\nSummarizing Violations\nGet an actionable summary of the most critical issues:\nsummary = summarize_violations(result, top_k=3)\n\nprint(f\"Overall status: {'PASS' if summary.ok else 'FAIL'}\")\nprint(f\"\\nTop {len(summary.top_issues)} critical issues:\")\nfor issue in summary.top_issues:\n    print(f\"  - {issue.column}: {issue.kind}\")\n\nprint(f\"\\nIssue counts by type:\")\nfor kind, count in summary.counts_by_kind.items():\n    print(f\"  {kind}: {count}\")\n\n\nComparing Contracts (Drift Detection)\nDetect changes between data versions:\n# New data with different characteristics\nnew_data = pd.DataFrame({\n    \"age\": [25, 40, 50, 60],  # Expanded age range\n    \"salary\": [50000, 75000, 90000, 110000],  # Higher salaries\n    \"department\": [\"HR\", \"Engineering\", \"Finance\", \"Engineering\"],  # New dept\n    \"location\": [\"NYC\", \"SF\", \"NYC\", \"Austin\"]  # New column\n})\n\nnew_contract = infer_contract(new_data)\ndrift_report = compare_contracts(contract, new_contract)\n\nprint(f\"Schema drift detected: {drift_report.has_schema_drift}\")\nprint(f\"Distribution drift detected: {drift_report.has_distribution_drift}\")\n\nif drift_report.added_columns:\n    print(f\"New columns: {drift_report.added_columns}\")\nif drift_report.distribution_changes:\n    print(f\"Distribution changes in: {list(drift_report.distribution_changes.keys())}\")",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#detailed-documentation",
    "href": "index.html#detailed-documentation",
    "title": "pyos_data_validation",
    "section": "Detailed Documentation",
    "text": "Detailed Documentation\nFor complete documentation of each function including all parameters, return types, and additional examples:\n\ninfer_contract - Learn data contracts from DataFrames\nvalidate_contract - Check data against contracts\n\ncompare_contracts - Detect schema and distribution drift\nsummarize_violations - Prioritize validation failures\n\nSee the complete API Reference for all functions and types.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "pyos_data_validation",
    "section": "Use Cases",
    "text": "Use Cases\n\nCI/CD Integration\nUse in automated testing:\ndef test_data_quality():\n    expected_contract = infer_contract(reference_data)\n    result = validate_contract(new_data, expected_contract)\n    \n    if not result.ok:\n        summary = summarize_violations(result)\n        raise AssertionError(\n            f\"Data validation failed with {len(summary.top_issues)} critical issues\"\n        )\n\n\nData Pipeline Monitoring\nMonitor data drift over time:\nbaseline_contract = infer_contract(baseline_data)\n\nfor batch in data_batches:\n    current_contract = infer_contract(batch)\n    drift = compare_contracts(baseline_contract, current_contract)\n    \n    if drift.has_schema_drift:\n        alert_team(\"Schema drift detected!\")\n    if drift.has_distribution_drift:\n        log_drift_metrics(drift.distribution_changes)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "pyos_data_validation",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions! Check out:\n\nContributing Guide\nDevelopment Guide\nCode of Conduct",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "pyos_data_validation",
    "section": "License",
    "text": "License\nMIT License - see LICENSE for details.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "reference/infer_contract.html",
    "href": "reference/infer_contract.html",
    "title": "infer_contract",
    "section": "",
    "text": "infer_contract\n\n\n\n\n\nName\nDescription\n\n\n\n\ninfer_contract\nDerive a data contract from a pandas DataFrame.\n\n\n\n\n\ninfer_contract.infer_contract(df)\nDerive a data contract from a pandas DataFrame.\nDerives per-column expectations—including expected data type, allowable missingness, optional numeric bounds, and optional categorical domains. The resulting contract defines the expected schema and validation constraints for future datasets, based on the observed structure of the input DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nA pandas DataFrame used to derive the data contract. This should be an example of “good” data that represents the expected structure and constraints of future datasets.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nContract\nA Contract object mapping column names to ColumnRule definitions, describing the expected schema and constraints of the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from data_validation.infer_contract import infer_contract\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"height\": [170.0, 180.5, 175.2],\n...     \"color\": [\"red\", \"blue\", \"red\"],\n... })\n&gt;&gt;&gt; contract = infer_contract(df)\n&gt;&gt;&gt; contract.name\n'contract'\n&gt;&gt;&gt; sorted(contract.columns.keys())\n['age', 'color', 'height']\n&gt;&gt;&gt; contract.columns[\"age\"].dtype\n'int'\n&gt;&gt;&gt; contract.columns[\"age\"].min_value &lt;= contract.columns[\"age\"].max_value\nTrue\n&gt;&gt;&gt; contract.columns[\"color\"].allowed_values == {\"red\", \"blue\"}\nTrue"
  },
  {
    "objectID": "reference/infer_contract.html#functions",
    "href": "reference/infer_contract.html#functions",
    "title": "infer_contract",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ninfer_contract\nDerive a data contract from a pandas DataFrame.\n\n\n\n\n\ninfer_contract.infer_contract(df)\nDerive a data contract from a pandas DataFrame.\nDerives per-column expectations—including expected data type, allowable missingness, optional numeric bounds, and optional categorical domains. The resulting contract defines the expected schema and validation constraints for future datasets, based on the observed structure of the input DataFrame.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nA pandas DataFrame used to derive the data contract. This should be an example of “good” data that represents the expected structure and constraints of future datasets.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nContract\nA Contract object mapping column names to ColumnRule definitions, describing the expected schema and constraints of the dataset.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from data_validation.infer_contract import infer_contract\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"height\": [170.0, 180.5, 175.2],\n...     \"color\": [\"red\", \"blue\", \"red\"],\n... })\n&gt;&gt;&gt; contract = infer_contract(df)\n&gt;&gt;&gt; contract.name\n'contract'\n&gt;&gt;&gt; sorted(contract.columns.keys())\n['age', 'color', 'height']\n&gt;&gt;&gt; contract.columns[\"age\"].dtype\n'int'\n&gt;&gt;&gt; contract.columns[\"age\"].min_value &lt;= contract.columns[\"age\"].max_value\nTrue\n&gt;&gt;&gt; contract.columns[\"color\"].allowed_values == {\"red\", \"blue\"}\nTrue"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "infer_contract\n\n\n\nvalidate_contract\n\n\n\ncompare_contracts\n\n\n\nsummarize_violations\n\n\n\n\n\n\n\n\n\n\ntypes.ColumnRule\nMinimal per-column expectations.\n\n\ntypes.Contract\nDataset contract = mapping of column name -&gt; ColumnRule.\n\n\ntypes.Issue\nA single validation issue.\n\n\ntypes.ValidationResult\nOutput of validate_contract().\n\n\ntypes.DriftReport\nOutput of compare_contracts().\n\n\ntypes.Summary\nOutput of summarize_violations().\n\n\ntypes.ContractViolationError\nRaised by validate_and_fail when validation fails.",
    "crumbs": [
      "Home",
      "API Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#core-functions",
    "href": "reference/index.html#core-functions",
    "title": "Function reference",
    "section": "",
    "text": "infer_contract\n\n\n\nvalidate_contract\n\n\n\ncompare_contracts\n\n\n\nsummarize_violations",
    "crumbs": [
      "Home",
      "API Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#types",
    "href": "reference/index.html#types",
    "title": "Function reference",
    "section": "",
    "text": "types.ColumnRule\nMinimal per-column expectations.\n\n\ntypes.Contract\nDataset contract = mapping of column name -&gt; ColumnRule.\n\n\ntypes.Issue\nA single validation issue.\n\n\ntypes.ValidationResult\nOutput of validate_contract().\n\n\ntypes.DriftReport\nOutput of compare_contracts().\n\n\ntypes.Summary\nOutput of summarize_violations().\n\n\ntypes.ContractViolationError\nRaised by validate_and_fail when validation fails.",
    "crumbs": [
      "Home",
      "API Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/types.Contract.html",
    "href": "reference/types.Contract.html",
    "title": "types.Contract",
    "section": "",
    "text": "types.Contract\ntypes.Contract(columns, name='contract')\nDataset contract = mapping of column name -&gt; ColumnRule."
  },
  {
    "objectID": "reference/types.ContractViolationError.html",
    "href": "reference/types.ContractViolationError.html",
    "title": "types.ContractViolationError",
    "section": "",
    "text": "types.ContractViolationError\ntypes.ContractViolationError()\nRaised by validate_and_fail when validation fails."
  },
  {
    "objectID": "reference/summarize_violations.html",
    "href": "reference/summarize_violations.html",
    "title": "summarize_violations",
    "section": "",
    "text": "summarize_violations\n\n\n\n\n\nName\nDescription\n\n\n\n\nsummarize_violations\nConvert a ValidationResult into an actionable summary.\n\n\n\n\n\nsummarize_violations.summarize_violations(result, *, top_k=5, weights=None)\nConvert a ValidationResult into an actionable summary.\nThis function takes a validation result and produces a concise summary highlighting the most critical issues. Issues are ranked by severity (using the kind-based weights) and counted by type.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nresult\nValidationResult\nThe validation result object to summarize, typically returned by the validate_contract function. Must contain a list of Issues.\nrequired\n\n\ntop_k\nint\nNumber of top violations to highlight (default is 5). Must be a positive integer. Set to a higher value to see more issues, or lower for a quick overview.\n5\n\n\nweights\ndict\nCustom weights for ranking violation severity by issue kind. Keys should be issue kinds (e.g., ‘missing_column’, ‘dtype’, ‘missingness’, ‘range’, ‘category’) and values should be positive numeric weights (int or float, higher = more severe). If None, default weights are used: - missing_column: 10 (most severe) - extra_column: 8 - dtype: 7 - range: 5 - category: 5 - missingness: 3 If a custom weights dict is provided, it completely replaces the default weights. Issue kinds not in the custom dict will default to weight 1. To override only specific kinds while keeping other defaults, explicitly include all desired weights.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSummary\nAn actionable summary containing: - ok : bool True if there are no issues (same as result.ok). - top_issues : List[Issue] The top_k most severe issues, ranked by weighted severity. If fewer than top_k issues exist, returns all issues. If no issues exist, returns an empty list. - counts_by_kind : Dict[str, int] Count of issues grouped by their kind (e.g., {‘dtype’: 2, ‘missingness’: 1, ‘range’: 3}). Empty dict if no issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf result is not a ValidationResult instance. If weights is not a dict or None.\n\n\n\nValueError\nIf top_k is not a positive integer. If weights contains non-numeric values. If weights contains negative or zero values.\n\n\n\n\n\n\nvalidate_contract : Validate a DataFrame against a contract. infer_contract : Infer a contract from a DataFrame.\n\n\n\nThe severity ranking helps users prioritize which issues to fix first. Schema issues (missing/extra columns, dtype mismatches) are typically weighted higher than distribution issues (ranges, categories) since they represent more fundamental problems.\nWhen multiple issues have the same weight, they are ordered by:\n\nColumn name (dataset-level issues with column=None sort first, then alphabetically)\nKind (alphabetically)\nOriginal order in result.issues\n\n\n\n\nBasic usage with default weights:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyos_data_validation import infer_contract, validate_contract, summarize_violations\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create training data to establish contract\n&gt;&gt;&gt; training_df = pd.DataFrame({\n...     \"age\": [25, 40, 35],\n...     \"salary\": [50000, 75000, 62000],\n...     \"department\": [\"HR\", \"Engineering\", \"Sales\"]\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Infer contract from training data\n&gt;&gt;&gt; contract = infer_contract(training_df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create test data with violations\n&gt;&gt;&gt; test_df = pd.DataFrame({\n...     \"age\": [30, 55, 22],  # 55 and 22 outside expected range\n...     \"salary\": [58000, 72000, 45000],  # 45000 below minimum\n...     \"department\": [\"HR\", \"Sales\", \"Marketing\"],  # Marketing not in contract\n...     \"bonus\": [5000, 8000, 3000]  # Extra column not in contract\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Validate and get issues\n&gt;&gt;&gt; result = validate_contract(test_df, contract)\n&gt;&gt;&gt; summary = summarize_violations(result, top_k=3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"Validation passed: {summary.ok}\")\nValidation passed: False\n&gt;&gt;&gt; print(f\"Found {len(summary.top_issues)} critical issues\")\nFound 3 critical issues\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Show top issues by severity\n&gt;&gt;&gt; for issue in summary.top_issues:\n...     print(f\"  - {issue.column}: {issue.kind}\")\n  - bonus: extra_column\n  - age: range\n  - age: range\nUsing counts for quick overview of issue types:\n&gt;&gt;&gt; print(summary.counts_by_kind)\n{'extra_column': 1, 'range': 3, 'category': 1}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check for critical schema issues\n&gt;&gt;&gt; if summary.counts_by_kind.get('missing_column', 0) &gt; 0:\n...     print(\"Critical: Missing required columns!\")\n&gt;&gt;&gt; if summary.counts_by_kind.get('extra_column', 0) &gt; 0:\n...     print(\"Warning: Extra columns detected\")\nWarning: Extra columns detected\nCustom weights to prioritize specific issue types:\n&gt;&gt;&gt; # Make range violations highest priority\n&gt;&gt;&gt; custom_summary = summarize_violations(\n...     result,\n...     top_k=5,\n...     weights={\n...         'missing_column': 10,\n...         'extra_column': 8,\n...         'dtype': 7,\n...         'range': 20,  # Highest priority!\n...         'category': 5,\n...         'missingness': 3\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Now range issues come first\n&gt;&gt;&gt; print(f\"Top issue type: {custom_summary.top_issues[0].kind}\")\nTop issue type: range\n&gt;&gt;&gt; print(f\"Top issue column: {custom_summary.top_issues[0].column}\")\nTop issue column: age\nHandling validation success (no issues):\n&gt;&gt;&gt; # Valid data that passes all checks\n&gt;&gt;&gt; valid_df = pd.DataFrame({\n...     \"age\": [30, 42],\n...     \"salary\": [58000, 72000],\n...     \"department\": [\"HR\", \"Sales\"]\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = validate_contract(valid_df, contract)\n&gt;&gt;&gt; summary = summarize_violations(result)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"Validation passed: {summary.ok}\")\nValidation passed: True\n&gt;&gt;&gt; print(f\"Issues found: {len(summary.top_issues)}\")\nIssues found: 0\n&gt;&gt;&gt; print(f\"Counts: {summary.counts_by_kind}\")\nCounts: {}"
  },
  {
    "objectID": "reference/summarize_violations.html#functions",
    "href": "reference/summarize_violations.html#functions",
    "title": "summarize_violations",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsummarize_violations\nConvert a ValidationResult into an actionable summary.\n\n\n\n\n\nsummarize_violations.summarize_violations(result, *, top_k=5, weights=None)\nConvert a ValidationResult into an actionable summary.\nThis function takes a validation result and produces a concise summary highlighting the most critical issues. Issues are ranked by severity (using the kind-based weights) and counted by type.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nresult\nValidationResult\nThe validation result object to summarize, typically returned by the validate_contract function. Must contain a list of Issues.\nrequired\n\n\ntop_k\nint\nNumber of top violations to highlight (default is 5). Must be a positive integer. Set to a higher value to see more issues, or lower for a quick overview.\n5\n\n\nweights\ndict\nCustom weights for ranking violation severity by issue kind. Keys should be issue kinds (e.g., ‘missing_column’, ‘dtype’, ‘missingness’, ‘range’, ‘category’) and values should be positive numeric weights (int or float, higher = more severe). If None, default weights are used: - missing_column: 10 (most severe) - extra_column: 8 - dtype: 7 - range: 5 - category: 5 - missingness: 3 If a custom weights dict is provided, it completely replaces the default weights. Issue kinds not in the custom dict will default to weight 1. To override only specific kinds while keeping other defaults, explicitly include all desired weights.\nNone\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nSummary\nAn actionable summary containing: - ok : bool True if there are no issues (same as result.ok). - top_issues : List[Issue] The top_k most severe issues, ranked by weighted severity. If fewer than top_k issues exist, returns all issues. If no issues exist, returns an empty list. - counts_by_kind : Dict[str, int] Count of issues grouped by their kind (e.g., {‘dtype’: 2, ‘missingness’: 1, ‘range’: 3}). Empty dict if no issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf result is not a ValidationResult instance. If weights is not a dict or None.\n\n\n\nValueError\nIf top_k is not a positive integer. If weights contains non-numeric values. If weights contains negative or zero values.\n\n\n\n\n\n\nvalidate_contract : Validate a DataFrame against a contract. infer_contract : Infer a contract from a DataFrame.\n\n\n\nThe severity ranking helps users prioritize which issues to fix first. Schema issues (missing/extra columns, dtype mismatches) are typically weighted higher than distribution issues (ranges, categories) since they represent more fundamental problems.\nWhen multiple issues have the same weight, they are ordered by:\n\nColumn name (dataset-level issues with column=None sort first, then alphabetically)\nKind (alphabetically)\nOriginal order in result.issues\n\n\n\n\nBasic usage with default weights:\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from pyos_data_validation import infer_contract, validate_contract, summarize_violations\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create training data to establish contract\n&gt;&gt;&gt; training_df = pd.DataFrame({\n...     \"age\": [25, 40, 35],\n...     \"salary\": [50000, 75000, 62000],\n...     \"department\": [\"HR\", \"Engineering\", \"Sales\"]\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Infer contract from training data\n&gt;&gt;&gt; contract = infer_contract(training_df)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Create test data with violations\n&gt;&gt;&gt; test_df = pd.DataFrame({\n...     \"age\": [30, 55, 22],  # 55 and 22 outside expected range\n...     \"salary\": [58000, 72000, 45000],  # 45000 below minimum\n...     \"department\": [\"HR\", \"Sales\", \"Marketing\"],  # Marketing not in contract\n...     \"bonus\": [5000, 8000, 3000]  # Extra column not in contract\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Validate and get issues\n&gt;&gt;&gt; result = validate_contract(test_df, contract)\n&gt;&gt;&gt; summary = summarize_violations(result, top_k=3)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"Validation passed: {summary.ok}\")\nValidation passed: False\n&gt;&gt;&gt; print(f\"Found {len(summary.top_issues)} critical issues\")\nFound 3 critical issues\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Show top issues by severity\n&gt;&gt;&gt; for issue in summary.top_issues:\n...     print(f\"  - {issue.column}: {issue.kind}\")\n  - bonus: extra_column\n  - age: range\n  - age: range\nUsing counts for quick overview of issue types:\n&gt;&gt;&gt; print(summary.counts_by_kind)\n{'extra_column': 1, 'range': 3, 'category': 1}\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Check for critical schema issues\n&gt;&gt;&gt; if summary.counts_by_kind.get('missing_column', 0) &gt; 0:\n...     print(\"Critical: Missing required columns!\")\n&gt;&gt;&gt; if summary.counts_by_kind.get('extra_column', 0) &gt; 0:\n...     print(\"Warning: Extra columns detected\")\nWarning: Extra columns detected\nCustom weights to prioritize specific issue types:\n&gt;&gt;&gt; # Make range violations highest priority\n&gt;&gt;&gt; custom_summary = summarize_violations(\n...     result,\n...     top_k=5,\n...     weights={\n...         'missing_column': 10,\n...         'extra_column': 8,\n...         'dtype': 7,\n...         'range': 20,  # Highest priority!\n...         'category': 5,\n...         'missingness': 3\n...     }\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Now range issues come first\n&gt;&gt;&gt; print(f\"Top issue type: {custom_summary.top_issues[0].kind}\")\nTop issue type: range\n&gt;&gt;&gt; print(f\"Top issue column: {custom_summary.top_issues[0].column}\")\nTop issue column: age\nHandling validation success (no issues):\n&gt;&gt;&gt; # Valid data that passes all checks\n&gt;&gt;&gt; valid_df = pd.DataFrame({\n...     \"age\": [30, 42],\n...     \"salary\": [58000, 72000],\n...     \"department\": [\"HR\", \"Sales\"]\n... })\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = validate_contract(valid_df, contract)\n&gt;&gt;&gt; summary = summarize_violations(result)\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(f\"Validation passed: {summary.ok}\")\nValidation passed: True\n&gt;&gt;&gt; print(f\"Issues found: {len(summary.top_issues)}\")\nIssues found: 0\n&gt;&gt;&gt; print(f\"Counts: {summary.counts_by_kind}\")\nCounts: {}"
  },
  {
    "objectID": "reference/types.Summary.html",
    "href": "reference/types.Summary.html",
    "title": "types.Summary",
    "section": "",
    "text": "types.Summary\ntypes.Summary(ok, top_issues=list(), counts_by_kind=dict())\nOutput of summarize_violations().\ncounts_by_kind is useful for CI logs (“dtype: 2, missingness: 1”)."
  }
]